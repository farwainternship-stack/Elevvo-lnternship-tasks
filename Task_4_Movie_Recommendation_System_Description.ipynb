{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip -q ml-100k.zip -d ./data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM8OGkB0Tl_h",
        "outputId": "cc7342ee-72a9-4e69-aa69-7ca2fec56b2c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-20 18:09:33--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.96.204\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.96.204|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.grouplens.org/datasets/movielens/ml-100k.zip [following]\n",
            "--2025-09-20 18:09:33--  https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.96.204|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip’\n",
            "\n",
            "ml-100k.zip         100%[===================>]   4.70M  21.9MB/s    in 0.2s    \n",
            "\n",
            "2025-09-20 18:09:33 (21.9 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip -q ml-100k.zip -d ./data\n",
        "!ls ./data/ml-100k | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enPPiE0iUVM3",
        "outputId": "f0da14b1-7e5d-4f21-d8f0-f9661f51b6dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-20 18:10:47--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.96.204\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.96.204|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://files.grouplens.org/datasets/movielens/ml-100k.zip [following]\n",
            "--2025-09-20 18:10:47--  https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.96.204|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4924029 (4.7M) [application/zip]\n",
            "Saving to: ‘ml-100k.zip.1’\n",
            "\n",
            "ml-100k.zip.1       100%[===================>]   4.70M  20.5MB/s    in 0.2s    \n",
            "\n",
            "2025-09-20 18:10:47 (20.5 MB/s) - ‘ml-100k.zip.1’ saved [4924029/4924029]\n",
            "\n",
            "replace ./data/ml-100k/allbut.pl? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace ./data/ml-100k/mku.sh? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace ./data/ml-100k/README? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace ./data/ml-100k/u.data? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace ./data/ml-100k/u.genre? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace ./data/ml-100k/u.info? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace ./data/ml-100k/u.item? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "allbut.pl\n",
            "mku.sh\n",
            "README\n",
            "u1.base\n",
            "u1.test\n",
            "u2.base\n",
            "u2.test\n",
            "u3.base\n",
            "u3.test\n",
            "u4.base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"./data/ml-100k\"\n"
      ],
      "metadata": {
        "id": "5lLTjxztU7Xc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip -qo ml-100k.zip -d ./data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcWsuWLCUVVW",
        "outputId": "49771a8b-f95c-4776-b444-a2dc8d9e17ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘ml-100k.zip’ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qo ml-100k.zip -d ./data\n"
      ],
      "metadata": {
        "id": "VcAOEOWgVC3d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./data/ml-100k | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go37YS3pVb8A",
        "outputId": "b2c4d4fb-6997-4e71-dcf7-af183d913a8e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "allbut.pl\n",
            "mku.sh\n",
            "README\n",
            "u1.base\n",
            "u1.test\n",
            "u2.base\n",
            "u2.test\n",
            "u3.base\n",
            "u3.test\n",
            "u4.base\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Movie Recommendation System (User-based CF) — Precision@K\n",
        "\n",
        "Follow the instructions from your assignment:\n",
        "- Build a recommender based on **user similarity** using a **user–item matrix**.\n",
        "- Recommend **top-rated unseen** movies for a given user.\n",
        "- **Evaluate** performance using **precision@K**.\n",
        "\n",
        "Extras included (optional):\n",
        "- Item-based CF recommender\n",
        "- Matrix factorization (Truncated SVD) baseline\n",
        "\n",
        "How to run (example):\n",
        "1) Download MovieLens 100K (either GroupLens `ml-100k` or a Kaggle CSV version) and unzip into a folder, e.g. `./data/ml-100k/`.\n",
        "2) Set DATA_DIR below to that folder.\n",
        "3) Run this script. It will:\n",
        "   - Autodetect the file format (Kaggle-style CSVs *or* original u.data/u.item)\n",
        "   - Train a user-based CF model on a per-user split\n",
        "   - Print **Precision@K**\n",
        "   - Print top-N recommendations for a sample user\n",
        "\n",
        "Tested with Python 3.10+, pandas, numpy, scikit-learn, scipy.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# ----------------------- CONFIG -----------------------\n",
        "DATA_DIR = \"./data/ml-100k\"  # fixed path for Colab/local run\n",
        "K = 10                       # for Precision@K and recommendation list length\n",
        "NEIGHBORS = 30               # number of nearest neighbors to use for scoring\n",
        "RATING_THRESHOLD = 4.0       # relevance threshold for evaluation\n",
        "TEST_SIZE_PER_USER = 0.2     # 20% per-user test split\n",
        "MIN_RATINGS_PER_USER = 5     # users with fewer are skipped in eval\n",
        "RNG_SEED = 42\n",
        "\n",
        "random.seed(RNG_SEED)\n",
        "np.random.seed(RNG_SEED)\n",
        "\n",
        "# ----------------------- DATA LOADER -----------------------\n",
        "\n",
        "def _load_kaggle_csv_style(path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Load Kaggle-style CSVs: ratings.csv (userId,movieId,rating,timestamp), movies.csv (movieId,title,genres).\"\"\"\n",
        "    ratings_path = os.path.join(path, \"ratings.csv\")\n",
        "    movies_path = os.path.join(path, \"movies.csv\")\n",
        "    if not (os.path.exists(ratings_path) and os.path.exists(movies_path)):\n",
        "        raise FileNotFoundError\n",
        "    ratings = pd.read_csv(ratings_path)\n",
        "    movies = pd.read_csv(movies_path)\n",
        "    # Standardize column names\n",
        "    ratings = ratings.rename(columns={\"user_id\": \"userId\", \"movie_id\": \"movieId\"})\n",
        "    movies = movies.rename(columns={\"movie_id\": \"movieId\", \"title_x\": \"title\"})\n",
        "    return ratings[[\"userId\", \"movieId\", \"rating\", \"timestamp\"]], movies[[\"movieId\", \"title\"]]\n",
        "\n",
        "\n",
        "def _load_original_u_files(path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Load original GroupLens `ml-100k` files: u.data (\\t-separated) and u.item (| separated).\"\"\"\n",
        "    udata = os.path.join(path, \"u.data\")\n",
        "    uitem = os.path.join(path, \"u.item\")\n",
        "    if not (os.path.exists(udata) and os.path.exists(uitem)):\n",
        "        raise FileNotFoundError\n",
        "    ratings = pd.read_csv(\n",
        "        udata,\n",
        "        sep=\"\\t\",\n",
        "        names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
        "        engine=\"python\",\n",
        "    )\n",
        "    movies = pd.read_csv(\n",
        "        uitem,\n",
        "        sep=\"|\",\n",
        "        names=[\n",
        "            \"movieId\", \"title\", \"release_date\", \"video_release_date\", \"IMDb_URL\",\n",
        "            \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\",\n",
        "            \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
        "            \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\",\n",
        "        ],\n",
        "        encoding_errors=\"ignore\",\n",
        "        engine=\"python\",\n",
        "    )\n",
        "    movies = movies[[\"movieId\", \"title\"]]\n",
        "    return ratings, movies\n",
        "\n",
        "\n",
        "def load_movielens_100k(path: str = DATA_DIR) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Try Kaggle CSV layout; fallback to GroupLens u.data/u.item.\"\"\"\n",
        "    try:\n",
        "        return _load_kaggle_csv_style(path)\n",
        "    except FileNotFoundError:\n",
        "        return _load_original_u_files(path)\n",
        "\n",
        "# ----------------------- UTILITIES -----------------------\n",
        "\n",
        "@dataclass\n",
        "class Encoders:\n",
        "    user2idx: Dict[int, int]\n",
        "    idx2user: List[int]\n",
        "    item2idx: Dict[int, int]\n",
        "    idx2item: List[int]\n",
        "\n",
        "\n",
        "def build_encoders(ratings: pd.DataFrame) -> Encoders:\n",
        "    users = np.sort(ratings.userId.unique())\n",
        "    items = np.sort(ratings.movieId.unique())\n",
        "    user2idx = {u: i for i, u in enumerate(users)}\n",
        "    item2idx = {m: i for i, m in enumerate(items)}\n",
        "    idx2user = users.tolist()\n",
        "    idx2item = items.tolist()\n",
        "    return Encoders(user2idx, idx2user, item2idx, idx2item)\n",
        "\n",
        "\n",
        "def df_to_csr(ratings: pd.DataFrame, enc: Encoders) -> csr_matrix:\n",
        "    rows = ratings.userId.map(enc.user2idx).values\n",
        "    cols = ratings.movieId.map(enc.item2idx).values\n",
        "    data = ratings.rating.values.astype(np.float32)\n",
        "    mat = csr_matrix((data, (rows, cols)), shape=(len(enc.idx2user), len(enc.idx2item)))\n",
        "    return mat\n",
        "\n",
        "# ----------------------- TRAIN/TEST SPLIT -----------------------\n",
        "\n",
        "def per_user_train_test_split(ratings: pd.DataFrame,\n",
        "                              test_size: float = TEST_SIZE_PER_USER,\n",
        "                              min_items: int = MIN_RATINGS_PER_USER) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Split each user's ratings into train/test (stratified by user). Uses timestamp if present, else random.\"\"\"\n",
        "    if \"timestamp\" not in ratings.columns:\n",
        "        ratings = ratings.assign(timestamp=0)\n",
        "    train_parts = []\n",
        "    test_parts = []\n",
        "    for uid, grp in ratings.groupby(\"userId\"):\n",
        "        if len(grp) < max(2, min_items):\n",
        "            # keep very small-user data entirely in train\n",
        "            train_parts.append(grp)\n",
        "            continue\n",
        "        grp = grp.sort_values(\"timestamp\")\n",
        "        n_test = max(1, int(round(len(grp) * test_size)))\n",
        "        test_idx = grp.index[-n_test:]\n",
        "        train_idx = grp.index[:-n_test]\n",
        "        train_parts.append(ratings.loc[train_idx])\n",
        "        test_parts.append(ratings.loc[test_idx])\n",
        "    train_df = pd.concat(train_parts).reset_index(drop=True)\n",
        "    test_df = pd.concat(test_parts).reset_index(drop=True) if test_parts else pd.DataFrame(columns=ratings.columns)\n",
        "    return train_df, test_df\n",
        "\n",
        "# ----------------------- USER-BASED CF -----------------------\n",
        "\n",
        "def user_similarity_matrix(R: csr_matrix) -> np.ndarray:\n",
        "    \"\"\"Cosine similarity between users (rows). Returns dense array of shape [n_users, n_users].\"\"\"\n",
        "    # Add tiny epsilon to avoid divide-by-zero within cosine\n",
        "    sims = cosine_similarity(R)\n",
        "    np.fill_diagonal(sims, 0.0)  # exclude self-similarity\n",
        "    return sims\n",
        "\n",
        "\n",
        "def score_items_for_user(user_idx: int, R: csr_matrix, sims: np.ndarray,\n",
        "                         neighbors: int = NEIGHBORS) -> np.ndarray:\n",
        "    \"\"\"Predict preference scores for **all items** for a target user (mean-centered, top-N neighbors).\"\"\"\n",
        "    r_u = R.getrow(user_idx).toarray().ravel()  # ratings by target user\n",
        "    # Select top neighbors by similarity\n",
        "    sim_vec = sims[user_idx]\n",
        "    if neighbors is not None and neighbors > 0:\n",
        "        top_idx = np.argpartition(-sim_vec, min(neighbors, len(sim_vec)-1))[:neighbors]\n",
        "        sim_vec = sim_vec[top_idx]\n",
        "        R_neighbors = R[top_idx]\n",
        "    else:\n",
        "        R_neighbors = R\n",
        "    # Weighted sum of neighbors' ratings\n",
        "    num = sim_vec @ R_neighbors.toarray()\n",
        "    den = np.abs(sim_vec).sum() + 1e-8\n",
        "    scores = num / den\n",
        "    # Don't recommend already-seen items\n",
        "    scores[r_u > 0] = -np.inf\n",
        "    return scores\n",
        "\n",
        "\n",
        "def recommend_for_user(user_external_id: int, enc: Encoders, R: csr_matrix, sims: np.ndarray,\n",
        "                        movies: pd.DataFrame, topk: int = K, neighbors: int = NEIGHBORS) -> pd.DataFrame:\n",
        "    uidx = enc.user2idx[user_external_id]\n",
        "    scores = score_items_for_user(uidx, R, sims, neighbors)\n",
        "    top_items = np.argpartition(-scores, topk)[:topk]\n",
        "    top_items = top_items[np.argsort(-scores[top_items])]\n",
        "    movie_ids = [enc.idx2item[i] for i in top_items]\n",
        "    titles = movies.set_index(\"movieId\").loc[movie_ids][\"title\"].values\n",
        "    return pd.DataFrame({\"movieId\": movie_ids, \"title\": titles, \"score\": scores[top_items]})\n",
        "\n",
        "# ----------------------- EVALUATION -----------------------\n",
        "\n",
        "def precision_at_k(recommended: List[int], relevant: set[int], k: int) -> float:\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "    hits = sum(1 for m in recommended[:k] if m in relevant)\n",
        "    return hits / k\n",
        "\n",
        "\n",
        "def evaluate_precision_at_k(train: pd.DataFrame, test: pd.DataFrame, enc: Encoders, movies: pd.DataFrame,\n",
        "                             topk: int = K, neighbors: int = NEIGHBORS,\n",
        "                             threshold: float = RATING_THRESHOLD) -> float:\n",
        "    if test.empty:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    R_train = df_to_csr(train, enc)\n",
        "    sims = user_similarity_matrix(R_train)\n",
        "\n",
        "    precisions = []\n",
        "    for uid, grp in test.groupby(\"userId\"):\n",
        "        # Only evaluate users that exist in train encoders\n",
        "        if uid not in enc.user2idx:\n",
        "            continue\n",
        "        # Relevant = items the user rated >= threshold **in the test set**\n",
        "        relevant = set(grp.loc[grp.rating >= threshold, \"movieId\"].tolist())\n",
        "        if not relevant:\n",
        "            continue\n",
        "        recs_df = recommend_for_user(uid, enc, R_train, sims, movies, topk=topk, neighbors=neighbors)\n",
        "        recommended = recs_df.movieId.tolist()\n",
        "        p = precision_at_k(recommended, relevant, topk)\n",
        "        precisions.append(p)\n",
        "    if not precisions:\n",
        "        return float(\"nan\")\n",
        "    return float(np.mean(precisions))\n",
        "\n",
        "# ----------------------- ITEM-BASED CF (Bonus) -----------------------\n",
        "\n",
        "def item_similarity_matrix(R: csr_matrix) -> np.ndarray:\n",
        "    sims = cosine_similarity(R.T)  # item–item\n",
        "    np.fill_diagonal(sims, 0.0)\n",
        "    return sims\n",
        "\n",
        "\n",
        "def recommend_item_based(user_external_id: int, enc: Encoders, R: csr_matrix, item_sims: np.ndarray,\n",
        "                         movies: pd.DataFrame, topk: int = K) -> pd.DataFrame:\n",
        "    uidx = enc.user2idx[user_external_id]\n",
        "    r_u = R.getrow(uidx).toarray().ravel()\n",
        "    scores = r_u @ item_sims  # sum of similarities to items the user liked\n",
        "    scores[r_u > 0] = -np.inf\n",
        "    top_items = np.argpartition(-scores, topk)[:topk]\n",
        "    top_items = top_items[np.argsort(-scores[top_items])]\n",
        "    movie_ids = [enc.idx2item[i] for i in top_items]\n",
        "    titles = movies.set_index(\"movieId\").loc[movie_ids][\"title\"].values\n",
        "    return pd.DataFrame({\"movieId\": movie_ids, \"title\": titles, \"score\": scores[top_items]})\n",
        "\n",
        "# ----------------------- SVD BASELINE (Bonus) -----------------------\n",
        "\n",
        "def svd_recommend(user_external_id: int, enc: Encoders, R: csr_matrix, n_components: int = 40, topk: int = K,\n",
        "                   movies: pd.DataFrame | None = None) -> pd.DataFrame:\n",
        "    svd = TruncatedSVD(n_components=n_components, random_state=RNG_SEED)\n",
        "    U = svd.fit_transform(R)           # users x comp\n",
        "    VT = svd.components_               # comp x items\n",
        "    preds = U @ VT                     # users x items\n",
        "    uidx = enc.user2idx[user_external_id]\n",
        "    r_u = R.getrow(uidx).toarray().ravel()\n",
        "    scores = preds[uidx].copy()\n",
        "    scores[r_u > 0] = -np.inf\n",
        "    top_items = np.argpartition(-scores, topk)[:topk]\n",
        "    top_items = top_items[np.argsort(-scores[top_items])]\n",
        "    if movies is None:\n",
        "        return pd.DataFrame({\"item_idx\": top_items, \"score\": scores[top_items]})\n",
        "    movie_ids = [enc.idx2item[i] for i in top_items]\n",
        "    titles = movies.set_index(\"movieId\").loc[movie_ids][\"title\"].values\n",
        "    return pd.DataFrame({\"movieId\": movie_ids, \"title\": titles, \"score\": scores[top_items]})\n",
        "\n",
        "# ----------------------- MAIN -----------------------\n",
        "\n",
        "def main():\n",
        "    print(\"Loading data from:\", DATA_DIR)\n",
        "    ratings, movies = load_movielens_100k(DATA_DIR)\n",
        "\n",
        "    print(\"Ratings:\", ratings.shape, \"Movies:\", movies.shape)\n",
        "    enc = build_encoders(ratings)\n",
        "\n",
        "    train_df, test_df = per_user_train_test_split(\n",
        "        ratings[[\"userId\", \"movieId\", \"rating\", \"timestamp\"]],\n",
        "        test_size=TEST_SIZE_PER_USER,\n",
        "        min_items=MIN_RATINGS_PER_USER,\n",
        "    )\n",
        "    print(f\"Train ratings: {len(train_df):,} | Test ratings: {len(test_df):,}\")\n",
        "\n",
        "    # Build train matrix & user similarities\n",
        "    R_train = df_to_csr(train_df, enc)\n",
        "    user_sims = user_similarity_matrix(R_train)\n",
        "\n",
        "    # Evaluate precision@K\n",
        "    p_at_k = evaluate_precision_at_k(train_df, test_df, enc, movies, topk=K, neighbors=NEIGHBORS,\n",
        "                                     threshold=RATING_THRESHOLD)\n",
        "    print(f\"\\nPrecision@{K}: {p_at_k:.4f}\")\n",
        "\n",
        "    # Show recommendations for a sample user that exists in train\n",
        "    sample_user = int(train_df.userId.sample(1, random_state=RNG_SEED).iloc[0])\n",
        "    print(f\"\\nSample recommendations for user {sample_user} (user-based CF):\")\n",
        "    recs = recommend_for_user(sample_user, enc, R_train, user_sims, movies, topk=K, neighbors=NEIGHBORS)\n",
        "    print(recs)\n",
        "\n",
        "    # --- Bonus demos (optional) ---\n",
        "    print(f\"\\n[Bonus] Item-based CF recommendations for user {sample_user}:\")\n",
        "    item_sims = item_similarity_matrix(R_train)\n",
        "    print(recommend_item_based(sample_user, enc, R_train, item_sims, movies, topk=K))\n",
        "\n",
        "    print(f\"\\n[Bonus] SVD baseline recommendations for user {sample_user}:\")\n",
        "    print(svd_recommend(sample_user, enc, R_train, n_components=40, topk=K, movies=movies))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmFSyZJSViPI",
        "outputId": "9dcaad08-734e-4bdb-9e39-dd934986ad5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from: ./data/ml-100k\n",
            "Ratings: (100000, 4) Movies: (1682, 2)\n",
            "Train ratings: 80,000 | Test ratings: 20,000\n",
            "\n",
            "Precision@10: 0.1294\n",
            "\n",
            "Sample recommendations for user 524 (user-based CF):\n",
            "   movieId                                   title     score\n",
            "0      427            To Kill a Mockingbird (1962)  3.961439\n",
            "1      357  One Flew Over the Cuckoo's Nest (1975)  3.892331\n",
            "2      479                          Vertigo (1958)  3.641742\n",
            "3       28                        Apollo 13 (1995)  3.500951\n",
            "4      496            It's a Wonderful Life (1946)  3.406243\n",
            "5      603                      Rear Window (1954)  3.300198\n",
            "6      176                           Aliens (1986)  3.193579\n",
            "7      183                            Alien (1979)  3.177843\n",
            "8      197                    Graduate, The (1967)  3.153037\n",
            "9        9                 Dead Man Walking (1995)  3.146633\n",
            "\n",
            "[Bonus] Item-based CF recommendations for user 524:\n",
            "   movieId                                   title       score\n",
            "0      183                            Alien (1979)  311.759064\n",
            "1      173              Princess Bride, The (1987)  309.636261\n",
            "2      202                    Groundhog Day (1993)  307.710815\n",
            "3      176                           Aliens (1986)  307.040924\n",
            "4      196               Dead Poets Society (1989)  303.531982\n",
            "5       28                        Apollo 13 (1995)  299.903107\n",
            "6      357  One Flew Over the Cuckoo's Nest (1975)  299.069153\n",
            "7       11                    Seven (Se7en) (1995)  294.534851\n",
            "8      197                    Graduate, The (1967)  294.521118\n",
            "9      655                      Stand by Me (1986)  292.529724\n",
            "\n",
            "[Bonus] SVD baseline recommendations for user 524:\n",
            "   movieId                                   title     score\n",
            "0      177  Good, The Bad and The Ugly, The (1966)  3.510607\n",
            "1      566         Clear and Present Danger (1994)  3.355168\n",
            "2       11                    Seven (Se7en) (1995)  3.330477\n",
            "3      428                 Harold and Maude (1971)  3.210583\n",
            "4      121           Independence Day (ID4) (1996)  3.178146\n",
            "5      188                Full Metal Jacket (1987)  3.077517\n",
            "6        8                             Babe (1995)  3.068782\n",
            "7       91  Nightmare Before Christmas, The (1993)  2.582167\n",
            "8      161                          Top Gun (1986)  2.371347\n",
            "9      510           Magnificent Seven, The (1954)  2.343697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ge1z0G0Vibq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "armjZcBcVic-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3k6e-zdGViho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SK9sq3vIVijO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JP0uxJGBVcWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}